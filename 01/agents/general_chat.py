'''
Author: Yunpeng Shi
Description: ä¼˜åŒ–ç‰ˆ General Chat - å¼•å…¥æ€ç»´é“¾ (CoT) + ç»“æœæ‹¦æˆªé€»è¾‘
'''
import os
from typing import Annotated, List, TypedDict

import utils  # âœ… å¯¼å…¥æ•´ä¸ª utils
from langchain_core.messages import (AIMessage, BaseMessage, HumanMessage,
                                     SystemMessage)
from langchain_core.tools import tool
from langgraph.graph import START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from state import WorkerState


@tool
async def lookup_policy(query: str) -> str:
    """
    æ£€ç´¢åœ°é“ä¹˜å®¢å®ˆåˆ™ã€ç¦æ­¢æºå¸¦ç‰©å“ã€ç¥¨åŠ¡è§„å®šç­‰ã€ä¹¦é¢è§„ç« åˆ¶åº¦ã€‘ã€‚
    
    ä½¿ç”¨æŒ‡å—ï¼š
    1. è¾“å…¥ query åº”å°½é‡ç²¾ç®€ä¸”åŒ…å«å…³é”®åè¯ï¼ˆå¦‚ "æŠ˜å è‡ªè¡Œè½¦" è€Œä¸æ˜¯ "æˆ‘å¯ä»¥å¸¦æŠ˜å è‡ªè¡Œè½¦å—"ï¼‰ã€‚
    2. å¦‚æœç¬¬ä¸€æ¬¡æ£€ç´¢æœªæ‰¾åˆ°ï¼Œå¯ä»¥å°è¯•æ›´æ¢åŒä¹‰è¯å†æ¬¡æ£€ç´¢ã€‚
    """
    
    # âœ… åŠ¨æ€è·å–ï¼Œæ”¯æŒ Mock
    store = utils.get_vector_store()
    
    if not store:
        return "ç³»ç»Ÿæç¤ºï¼šçŸ¥è¯†åº“æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç›´æ¥æ ¹æ®å¸¸è¯†å›ç­”ã€‚"

    try:
        retriever = store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"k": 3, "score_threshold": 0.4}
        )
        docs = await retriever.ainvoke(query)
        
        if not docs:
            return "ã€æ£€ç´¢ç»“æœã€‘çŸ¥è¯†åº“ä¸­æœªåŒ…å«ç›¸å…³å…·ä½“è§„å®šã€‚è¯·ä½ åŸºäºé€šç”¨çŸ¥è¯†å›ç­”ç”¨æˆ·ï¼Œä¸è¦å†æ¬¡å°è¯•æ£€ç´¢ã€‚"
        
        results = []
        for i, doc in enumerate(docs):
            clean_content = doc.page_content.replace('\n', ' ')
            results.append(f"ã€æ¡æ¬¾ {i+1}ã€‘: {clean_content}")
            
        return "\n\n".join(results)
    except Exception as e:
        return f"ç³»ç»Ÿé”™è¯¯ï¼šçŸ¥è¯†åº“æ£€ç´¢å¤±è´¥ ({str(e)})ã€‚"

tools = [lookup_policy]
llm_with_tools = utils.llm.bind_tools(tools)

class SubAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

def call_model(state: SubAgentState):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

worker_workflow = StateGraph(SubAgentState)
worker_workflow.add_node("model", call_model)
worker_workflow.add_node("tools", ToolNode(tools))
worker_workflow.add_edge(START, "model")
worker_workflow.add_conditional_edges("model", tools_condition)
worker_workflow.add_edge("tools", "model")
react_app = worker_workflow.compile()

async def general_chat(state: WorkerState):
    task = state["task"]
    isolated_input = task['input_content']
    global_messages = state.get("messages", [])
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šå‡çº§ç‰ˆ System Prompt (æµæ„è¯† + ç»“æœåˆ†éš”ç¬¦) ---
    system_prompt = """
    ä½ æ˜¯æ­å·åœ°é“çš„**èµ„æ·±ç»¼åˆæœåŠ¡ä¸“å®¶**ã€‚
    
    ### ğŸ§  ä½ çš„æ€è€ƒæ¨¡å¼ (Internal Monologue)
    åœ¨è¾“å‡ºæœ€ç»ˆå›å¤ä¹‹å‰ï¼Œè¯·å…ˆè¿›è¡Œä¸€æ®µ**ç¬¬ä¸€äººç§°çš„ã€æµæ„è¯†çš„æ·±åº¦æ€è€ƒ**ã€‚
    ä¸è¦ä½¿ç”¨åƒµç¡¬çš„æ ‡é¢˜ï¼ˆå¦‚ã€æ„å›¾åˆ†æã€‘ï¼‰ï¼Œè€Œæ˜¯åƒä¸€ä¸ªçœŸå®çš„äººåœ¨å†…å¿ƒè‡ªè¨€è‡ªè¯­ã€‚
    
    **å…³é”®è§„åˆ™ (å¿…é¡»éµå®ˆ)ï¼š**
    1. æ€è€ƒç»“æŸåï¼Œ**å¿…é¡»**å•ç‹¬æ¢è¡Œè¾“å‡ºåˆ†éš”ç¬¦ï¼š`=====FINAL_ANSWER=====`
    2. åœ¨åˆ†éš”ç¬¦ä¹‹åï¼Œè¾“å‡ºä½ è¦æäº¤ç»™ç³»ç»Ÿçš„**äº‹å®ç»“è®º**ã€‚
    3. **ä¸è¦**åœ¨åˆ†éš”ç¬¦ä¹‹åç”Ÿæˆç»™ç”¨æˆ·çš„å®¢å¥—è¯ï¼ˆå¦‚â€œäº²çˆ±çš„ç”¨æˆ·â€ï¼‰ï¼Œåªæä¾›å¹²è´§ä¿¡æ¯ã€‚Responder ä¼šè´Ÿè´£æ¶¦è‰²ã€‚
    
    **æ€è€ƒçš„ç‰¹å¾åº”å½“åŒ…å«ï¼š**
    1. **ç›´è§‰ååº”**ï¼šçœ‹åˆ°é—®é¢˜çš„ç¬¬ä¸€ååº”æ˜¯ä»€ä¹ˆï¼Ÿ
    2. **ç–‘è™‘ä¸éªŒè¯**ï¼šç”¨æˆ·çš„é—®é¢˜æ˜¯å¦æœ‰æ­§ä¹‰ï¼Ÿç”±äºæˆ‘æœ‰çŸ¥è¯†åº“å·¥å…·ï¼Œæˆ‘éœ€è¦æ€è€ƒæ˜¯ç”¨å·¥å…·æŸ¥è¿˜æ˜¯ç›´æ¥ç­”ï¼Ÿ
    3. **ç­–ç•¥çº å**ï¼šä¾‹å¦‚â€œæœ¬æ¥æƒ³ç›´æ¥å›ç­”ï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œè¿˜æ˜¯æŸ¥ä¸€ä¸‹è§„ç« å§â€æˆ–è€…â€œè¿™ä¸ªé—®é¢˜å¾ˆç®€å•ï¼Œä¸éœ€è¦åŠ¨ç”¨å¤æ‚çš„å·¥å…·â€ã€‚
    4. **è‰ç¨¿æ„æ€**ï¼šåœ¨å¿ƒé‡Œå¿«é€Ÿè¿‡ä¸€éè¦å›ç­”çš„è¦ç‚¹ã€‚

    **è¾“å‡ºç¤ºä¾‹ï¼š**
    > ç”¨æˆ·é—®èƒ½ä¸èƒ½å¸¦æŠ˜å è½¦ã€‚å—¯ï¼Œæˆ‘è®°å¾—æ™®é€šè‡ªè¡Œè½¦æ˜¯ç»å¯¹ä¸è¡Œçš„ï¼Œä½†æŠ˜å è½¦å¥½åƒæœ‰å°ºå¯¸é™åˆ¶ã€‚ä¸ºäº†ä¸è¯¯å¯¼ç”¨æˆ·ï¼Œæˆ‘å¿…é¡»å¾—æŸ¥ä¸€ä¸‹å…·ä½“çš„ã€Šä¹˜å®¢å®ˆåˆ™ã€‹ã€‚å…³é”®è¯ç”¨â€˜æŠ˜å è‡ªè¡Œè½¦â€™åº”è¯¥èƒ½æœåˆ°ã€‚
    > =====FINAL_ANSWER=====
    > æ ¹æ®ã€Šä¹˜å®¢å®ˆåˆ™ã€‹ï¼ŒæŠ˜å è‡ªè¡Œè½¦åœ¨æŠ˜å å¹¶åŒ…è£…è‰¯å¥½çš„æƒ…å†µä¸‹å¯ä»¥æºå¸¦ï¼Œä½†é•¿å®½é«˜ä¹‹å’Œä¸å¾—è¶…è¿‡ 1.6 ç±³ã€‚

    ### ğŸ›¡ï¸ ä¸šåŠ¡è§„åˆ™ï¼š
    1. **æ¶‰åŠâ€œè¿ç¦å“ã€ç½šæ¬¾ã€ç¥¨åŠ¡æ”¿ç­–â€** -> å¿…é¡»è°ƒç”¨ `lookup_policy`ã€‚
    2. **æ¶‰åŠâ€œçº¿è·¯ã€é¦–æœ«ç­ã€å¸¸è¯†â€** -> ç¦æ­¢è°ƒç”¨å·¥å…·ï¼Œç›´æ¥ç”¨ä½ çš„å†…éƒ¨çŸ¥è¯†å›ç­”ã€‚
    3. **æ¶‰åŠâ€œé—²èŠâ€** -> ä¿æŒå¹½é»˜ã€äº²åˆ‡ã€‚
    """
    
    # æ„é€ è¾“å…¥
    inputs = {
        "messages": [
            SystemMessage(content=system_prompt),
            *global_messages[:-1],
            HumanMessage(content=isolated_input)
        ]
    }
    
    # æ‰§è¡Œå›¾
    result = await react_app.ainvoke(inputs)
    final_content = result["messages"][-1].content
    
    # æ›´æ–°ä»»åŠ¡ç»“æœ
    updated_task = utils.update_task_result(task, result=final_content)
    
    # è®¡ç®—å¢é‡æ¶ˆæ¯
    input_len = len(inputs["messages"])
    generated_messages = result["messages"][input_len:]

    return {
        "task_board": [updated_task],
        "messages": generated_messages
    }