'''
Author: Yunpeng Shi
Description: å·¥ä¸šçº§æ”¹é€  - å¢å¼ºå‹è·¯ç”±é€»è¾‘ (å½»åº•ä¿®å¤ Responder å†…å®¹æ³„éœ²åˆ°æ€è€ƒåŒºçš„é—®é¢˜)
'''
import asyncio
import json
import os
import time
from contextlib import asynccontextmanager
from typing import Any, AsyncGenerator, Dict, List

# ä¿æŒåŸæ¥çš„å¯¼å…¥ä¸å˜
from agents.complaint_agent import complaint_agent
from agents.general_chat import general_chat
from agents.judge_agent import judge_agent
from agents.manager_agent import manager_agent
from agents.responder_agent import responder_agent
from agents.supervisor import supervisor_node, workflow_router
from agents.ticket_agent import ticket_agent
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.graph import END, START, StateGraph
from psycopg_pool import AsyncConnectionPool
from pydantic import BaseModel
from state import agentState
from utils import logger

load_dotenv()

def format_sse(event_type: str, data: dict) -> str:
    return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"

DB_URI = os.getenv("DB_URI", "postgresql://user:password@localhost:5432/metro_agent_db")

# --- 1. æ„å»ºæ™ºèƒ½ä½“å›¾ ---
def build_graph():
    workflow = StateGraph(agentState)
    workflow.add_node("supervisor_node", supervisor_node)
    workflow.add_node("ticket_agent", ticket_agent)
    workflow.add_node("complaint_agent", complaint_agent)
    workflow.add_node("general_chat", general_chat)
    workflow.add_node("manager_agent", manager_agent)
    workflow.add_node("judge_agent", judge_agent)
    workflow.add_node("responder_agent", responder_agent) # å…³é”®èŠ‚ç‚¹åç§°

    workflow.add_edge(START, 'supervisor_node')
    workflow.add_conditional_edges(
        "supervisor_node",
        workflow_router,
        ["ticket_agent", "complaint_agent", "general_chat", "manager_agent", "judge_agent", "responder_agent"]
    )
    workflow.add_edge("ticket_agent", "supervisor_node")
    workflow.add_edge("complaint_agent", "supervisor_node")
    workflow.add_edge("general_chat", "supervisor_node")
    workflow.add_edge("manager_agent", "supervisor_node")
    workflow.add_edge("judge_agent", "supervisor_node")
    workflow.add_edge("responder_agent", END)
    return workflow

# --- 2. ç”Ÿå‘½å‘¨æœŸ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(">>> æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
    async with AsyncConnectionPool(conninfo=DB_URI, max_size=20, kwargs={"autocommit": True}) as pool:
        app.state.pool = pool
        async with pool.connection() as conn:
            checkpointer = AsyncPostgresSaver(conn)
            await checkpointer.setup()
        logger.info(">>> æœåŠ¡å¯åŠ¨æˆåŠŸï¼Œè·¯ç”±å·²å°±ç»ªã€‚")
        yield
    logger.info(">>> æœåŠ¡å·²åœæ­¢ã€‚")

app = FastAPI(title="Metro AI Agent Service", version="1.0.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    query: str
    thread_id: str = "default_thread"

class RenameRequest(BaseModel):
    title: str

# èŠ‚ç‚¹ä¸­æ–‡æ˜ å°„ï¼ˆç”¨äºå‰ç«¯ Step å±•ç¤ºï¼‰
NODE_DISPLAY_NAMES = {
    "supervisor_node": "æ€»æ§è°ƒåº¦ä¸­",
    "ticket_agent": "æ­£åœ¨æŸ¥è¯¢ç¥¨åŠ¡ç³»ç»Ÿ",
    "complaint_agent": "æ­£åœ¨å¤„ç†å»ºè®®åé¦ˆ",
    "general_chat": "æ­£åœ¨æ€è€ƒ",
    "manager_agent": "æ­£åœ¨æŸ¥é˜…ç®¡ç†æ‰‹å†Œ",
    "judge_agent": "æ­£åœ¨æŸ¥è¯¢è§„ç« åˆ¶åº¦",
    "responder_agent": "æ­£åœ¨æ•´ç†å›å¤",
}

# --- 3. æ ¸å¿ƒ API ---

@app.get("/health")
def health_check():
    return {"status": "ok", "db": "connected"}

@app.get("/threads")
async def list_threads():
    try:
        async with app.state.pool.connection() as conn:
            async with conn.cursor() as cur:
                query = """
                SELECT c.thread_id, COALESCE(m.title, c.thread_id) as title
                FROM (SELECT DISTINCT thread_id FROM checkpoints) c
                LEFT JOIN thread_metadata m ON c.thread_id = m.thread_id
                ORDER BY c.thread_id DESC
                """
                await cur.execute(query)
                rows = await cur.fetchall()
                return [{"thread_id": row[0], "title": row[1]} for row in rows]
    except Exception as e:
        logger.error(f"è·å–åˆ—è¡¨å¤±è´¥: {e}")
        return []

@app.get("/threads/{thread_id}/history")
async def get_history(thread_id: str):
    """å†å²è®°å½•é€»è¾‘ä¿æŒä¸å˜"""
    try:
        async with app.state.pool.connection() as conn:
            checkpointer = AsyncPostgresSaver(conn)
            graph_app = build_graph().compile(checkpointer=checkpointer)
            config = {"configurable": {"thread_id": thread_id}}
            
            state = await graph_app.aget_state(config)
            messages = state.values.get("messages", [])
            
            history = []
            current_ai_msg = None

            def get_val(obj, key, default=None):
                if isinstance(obj, dict): return obj.get(key, default)
                return getattr(obj, key, default)

            is_intermediate = [False] * len(messages)
            for i in range(len(messages)):
                m_type = get_val(messages[i], "type")
                if m_type in ("ai", "assistant"):
                    if i + 1 < len(messages):
                        next_type = get_val(messages[i+1], "type")
                        if next_type in ("ai", "assistant", "tool"):
                            is_intermediate[i] = True
                    
                    m_meta = get_val(messages[i], "metadata", {}) or get_val(messages[i], "response_metadata", {})
                    node = m_meta.get("langgraph_node", "")
                    if node and node != "responder_agent":
                        is_intermediate[i] = True
                    m_name = get_val(messages[i], "name", "")
                    if m_name and m_name != "responder_agent":
                        is_intermediate[i] = True

            for i, msg in enumerate(messages):
                m_type = get_val(msg, "type")
                m_content = get_val(msg, "content", "")

                if m_type in ("human", "user"):
                    if current_ai_msg:
                        history.append(current_ai_msg)
                        current_ai_msg = None
                    history.append({"role": "user", "content": m_content})
                
                elif m_type in ("ai", "assistant"):
                    if not current_ai_msg:
                        current_ai_msg = {
                            "role": "assistant", "content": "", "thoughts": "",
                            "steps": [], "hasThought": False, "isDoneThinking": True, 
                            "isThoughtExpanded": False 
                        }

                    tool_calls = get_val(msg, "tool_calls", []) or get_val(msg, "additional_kwargs", {}).get("tool_calls", [])
                    if tool_calls:
                        current_ai_msg["hasThought"] = True
                        for tc in tool_calls:
                            name = tc.get("function", {}).get("name") if isinstance(tc, dict) else getattr(tc, "name", "unknown")
                            current_ai_msg["steps"].append({"title": f"è°ƒç”¨å·¥å…·: {name}", "status": "done"})

                    if is_intermediate[i]:
                        if m_content:
                            current_ai_msg["hasThought"] = True
                            current_ai_msg["thoughts"] += str(m_content) + "\n"
                    else:
                        if m_content:
                            current_ai_msg["content"] += str(m_content)

                    reasoning = get_val(msg, "additional_kwargs", {}).get("reasoning_content", "")
                    if reasoning:
                        current_ai_msg["hasThought"] = True
                        current_ai_msg["thoughts"] += str(reasoning) + "\n"

                elif m_type == "tool":
                    if current_ai_msg:
                        current_ai_msg["hasThought"] = True

            if current_ai_msg:
                history.append(current_ai_msg)

            return {"history": history}

    except Exception as e:
        logger.error(f"è·å–å†å²å¤±è´¥: {e}")
        return {"history": []}

@app.post("/threads/{thread_id}/rename")
async def rename_thread(thread_id: str, request: RenameRequest):
    try:
        async with app.state.pool.connection() as conn:
            async with conn.cursor() as cur:
                await cur.execute(
                    """
                    INSERT INTO thread_metadata (thread_id, title) VALUES (%s, %s)
                    ON CONFLICT (thread_id) DO UPDATE SET title = EXCLUDED.title
                    """,
                    (thread_id.strip(), request.title)
                )
                return {"status": "success"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/threads/{thread_id}")
async def delete_thread(thread_id: str):
    try:
        async with app.state.pool.connection() as conn:
            async with conn.cursor() as cur:
                await cur.execute("DELETE FROM thread_metadata WHERE thread_id = %s", (thread_id,))
                await cur.execute("DELETE FROM checkpoint_writes WHERE thread_id = %s", (thread_id,))
                await cur.execute("DELETE FROM checkpoint_blobs WHERE thread_id = %s", (thread_id,))
                await cur.execute("DELETE FROM checkpoints WHERE thread_id = %s", (thread_id,))
                return {"status": "success"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# âš ï¸ æ ¸å¿ƒæµå¼æ¥å£ - ä¿®å¤ç‰ˆ
# ============================================================================
@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    async def event_generator():
        try:
            async with app.state.pool.connection() as conn:
                checkpointer = AsyncPostgresSaver(conn)
                graph_app = build_graph().compile(checkpointer=checkpointer)
                config = {"configurable": {"thread_id": request.thread_id}}
                input_state = {"messages": [HumanMessage(content=request.query)]}
                
                active_steps = set()

                async for event in graph_app.astream_events(input_state, config=config, version="v2"):
                    kind = event["event"]
                    name = event.get("name", "")
                    
                    # --- å¢å¼ºå‹èŠ‚ç‚¹èº«ä»½è¯†åˆ« ---
                    # 1. è·å– Metadata ä¸­çš„èŠ‚ç‚¹å
                    meta = event.get("metadata", {})
                    node_from_meta = meta.get("langgraph_node", "")
                    
                    # 2. è·å– Tags ä¸­çš„æ ‡ç­¾ (LangGraph é€šå¸¸ä¼šæŠŠ node name æ”¾å…¥ tags)
                    tags = event.get("tags", [])
                    
                    # 3. åˆ¤å®šæ˜¯å¦ä¸º responder_agent (åªè¦æ»¡è¶³å…¶ä¸€å³å¯)
                    is_responder = (node_from_meta == "responder_agent") or ("responder_agent" in tags)

                    # --- Step 1: å¤„ç†èŠ‚ç‚¹çŠ¶æ€ (UI Loading æ•ˆæœ) ---
                    if kind == "on_chain_start" and name in NODE_DISPLAY_NAMES:
                        # Responder èŠ‚ç‚¹æœ¬èº«ä¸æ˜¾ç¤ºä¸º"æ€è€ƒæ­¥éª¤"ï¼Œå®ƒæ˜¯æ­£åœ¨ç”Ÿæˆå›å¤
                        if name != "responder_agent":
                            step_title = NODE_DISPLAY_NAMES.get(name, f"æ­£åœ¨è¿è¡Œ {name}")
                            yield format_sse("step", {
                                "title": f"{step_title}...",
                                "status": "loading"
                            })
                            active_steps.add(name)

                    elif kind == "on_chain_end" and name in active_steps:
                        step_title = NODE_DISPLAY_NAMES.get(name, name)
                        final_title = step_title.replace("æ­£åœ¨", "") + " å®Œæˆ"
                        yield format_sse("step", {
                            "title": final_title,
                            "status": "done"
                        })
                        active_steps.remove(name)

                    # --- Step 2: å¤„ç†å·¥å…·è°ƒç”¨ ---
                    elif kind == "on_tool_start" and name != "FinalAnswer":
                        yield format_sse("step", {
                            "title": f"æ­£åœ¨è°ƒç”¨å·¥å…·: {name}...",
                            "status": "loading"
                        })
                    elif kind == "on_tool_end" and name != "FinalAnswer":
                        yield format_sse("step", {
                            "title": f"å·¥å…· {name} è°ƒç”¨å®Œæˆ",
                            "status": "done"
                        })
                    
                    # --- Step 3: æ ¸å¿ƒæ–‡æœ¬åˆ†æµé€»è¾‘ ---
                    elif kind == "on_chat_model_stream":
                        chunk = event["data"]["chunk"]
                        
                        # A. ä¼˜å…ˆå¤„ç† DeepSeek é£æ ¼çš„åŸç”Ÿæ€è€ƒå†…å®¹
                        reasoning = chunk.additional_kwargs.get("reasoning_content", "")
                        if reasoning:
                             yield format_sse("thought", {"content": reasoning})

                        # B. å¤„ç†æ­£æ–‡å†…å®¹ (Content)
                        if chunk.content:
                            # ğŸš¨ ä¸¥æ ¼çš„åˆ†æµåˆ¤æ–­ ğŸš¨
                            if is_responder:
                                # åªæœ‰ç¡®è®¤æ˜¯ Responderï¼Œæ‰å‘ç»™èŠå¤©æ°”æ³¡
                                yield format_sse("message", {"content": chunk.content})
                            else:
                                # å…¶ä»–æ‰€æœ‰èŠ‚ç‚¹çš„è¾“å‡ºï¼Œå…¨éƒ¨å½’ç±»ä¸º"æ€è€ƒè¿‡ç¨‹"
                                # è¿™æ · JudgeAgent/Supervisor çš„ä¸­é—´ç»“æœå°±ä¼šè¿›å…¥æŠ˜å æ¡†
                                yield format_sse("thought", {"content": chunk.content})

                # --- æ ‡é¢˜ç”Ÿæˆé€»è¾‘ (ä¿æŒä¸å˜) ---
                final_state = await graph_app.aget_state(config)
                messages = final_state.values.get("messages", [])
                if len(messages) > 0:
                    first_question = ""
                    first_answer = ""
                    for msg in messages:
                        if isinstance(msg, HumanMessage) and not first_question:
                            first_question = msg.content
                        elif isinstance(msg, AIMessage) and not first_answer and msg.content:
                            first_answer = msg.content
                    if first_question and first_answer:
                        from utils import llm
                        prompt = f"è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯æå–ä¸è¶…è¿‡10ä¸ªå­—çš„ç®€çŸ­æ ‡é¢˜ï¼š\né—®ï¼š{first_question[:50]}\nç­”ï¼š{first_answer[:50]}"
                        try:
                            # ä½¿ç”¨éæµå¼è°ƒç”¨é¿å…å¹²æ‰°
                            generated_title_msg = await llm.ainvoke([HumanMessage(content=prompt)])
                            title = generated_title_msg.content.strip().replace('"', '')
                            async with conn.cursor() as cur:
                                await cur.execute(
                                    "INSERT INTO thread_metadata (thread_id, title) VALUES (%s, %s) ON CONFLICT (thread_id) DO UPDATE SET title = EXCLUDED.title",
                                    (request.thread_id, title)
                                )
                            yield format_sse("title_generated", {"title": title, "thread_id": request.thread_id})
                        except Exception:
                            pass

                yield format_sse("done", "[DONE]")

        except Exception as e:
            logger.error(f"æµå¼å¼‚å¸¸: {e}")
            yield format_sse("error", {"error": str(e)})

    return StreamingResponse(event_generator(), media_type="text/event-stream")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)