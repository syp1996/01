import json
import os
import sys
from typing import Dict, List

from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_milvus import Milvus
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# --- é…ç½® ---
RAW_DOCS_DIR = "./data/raw_docs"
COLLECTION_NAME = "metro_knowledge" 
LOG_FILE = "./data/indexed_files.json"
# æŒ‡å‘åˆšæ‰ä¸‹è½½çš„æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„
LOCAL_MODEL_PATH = "./models/bge-small-zh-v1.5" 

# Milvus é…ç½®
MILVUS_HOST = "127.0.0.1"
MILVUS_PORT = 29530 

def load_processed_log() -> Dict[str, float]:
    if os.path.exists(LOG_FILE):
        try:
            with open(LOG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_processed_log(log_data: Dict[str, float]):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)

def get_all_files(directory: str, ext: str = ".txt") -> List[str]:
    file_paths = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(ext):
                file_paths.append(os.path.join(root, file))
    return file_paths

def build_index():
    # ==========================================
    # 0. ç¯å¢ƒæ¸…ç† (æœ€å…ˆæ‰§è¡Œï¼Œé˜²æ­¢å¹²æ‰°)
    # ==========================================
    print(">>> [Phase 0] æ¸…ç†ç½‘ç»œä»£ç†é…ç½®...")
    # å¼ºåŠ›æ¸…ç†æ‰€æœ‰ä»£ç†å˜é‡
    for key in ["http_proxy", "https_proxy", "all_proxy", "HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "grpc_proxy", "GRPC_PROXY"]:
        if key in os.environ:
            del os.environ[key]
    
    # è®¾ç½®ä¸èµ°ä»£ç†çš„åå•
    os.environ["NO_PROXY"] = "localhost,127.0.0.1,0.0.0.0,::1"
    print("    - ä»£ç†ç¯å¢ƒå˜é‡å·²æ¸…ç†ï¼Œç¡®ä¿ç›´è¿ Dockerã€‚")

    # ==========================================
    # 1. åŠ è½½æœ¬åœ°æ¨¡å‹ (ä¸è”ç½‘)
    # ==========================================
    print(f">>> [Phase 1] æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {LOCAL_MODEL_PATH}")
    
    if not os.path.exists(LOCAL_MODEL_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶å¤¹ {LOCAL_MODEL_PATH}")
        print("   è¯·å…ˆè¿è¡Œ download_model.py ä¸‹è½½æ¨¡å‹ï¼")
        return

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=LOCAL_MODEL_PATH, # ğŸ‘ˆ ç›´æ¥ä¼ æ–‡ä»¶å¤¹è·¯å¾„
            model_kwargs={'device': 'cpu'}, 
            encode_kwargs={'normalize_embeddings': True}
        )
        print(">>> âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f">>> âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # ==========================================
    # 2. å¤„ç†æ–‡ä»¶
    # ==========================================
    processed_log = load_processed_log()
    current_files = get_all_files(RAW_DOCS_DIR)
    new_files = []
    updated_log = processed_log.copy()

    for file_path in current_files:
        mtime = os.path.getmtime(file_path)
        file_name = os.path.relpath(file_path, RAW_DOCS_DIR)
        if file_name not in processed_log: # ç®€å•é€»è¾‘ï¼šåªçœ‹æ–‡ä»¶åæ˜¯å¦è®°å½•è¿‡
            new_files.append(file_path)
            updated_log[file_name] = mtime

    if not new_files:
        print(">>> æ²¡æœ‰å‘ç°æ–°æ–‡ä»¶ï¼Œæ— éœ€æ›´æ–°ã€‚")
        return

    print(f">>> å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†...")
    
    docs = []
    for file_path in new_files:
        try:
            loader = TextLoader(file_path, encoding="utf-8")
            loaded_docs = loader.load()
            # ä¼˜åŒ–ï¼šå¢åŠ  source å…ƒæ•°æ®
            for doc in loaded_docs:
                doc.metadata["source_filename"] = os.path.basename(file_path)
            docs.extend(loaded_docs)
        except Exception as e:
            print(f"    x è¯»å–å¤±è´¥: {file_path}, {e}")

    if not docs:
        return

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600, chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
    )
    splits = text_splitter.split_documents(docs)
    print(f">>> åˆ‡åˆ†å®Œæˆï¼Œå…± {len(splits)} ä¸ªåˆ‡ç‰‡ã€‚")

    # ==========================================
    # 3. æ¨é€åˆ° Milvus
    # ==========================================
    
    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šURI æ ¼å¼å¿…é¡»å¸¦ http://
    milvus_uri = f"tcp://{MILVUS_HOST}:{MILVUS_PORT}" 
    
    print(f">>> æ­£åœ¨è¿æ¥ Milvus: {milvus_uri}")

    try:
        Milvus.from_documents(
            splits,
            embeddings,
            collection_name=COLLECTION_NAME,
            connection_args={
                "uri": milvus_uri,  # ç»“æœ: tcp://127.0.0.1:29530
                "token": "",
                "timeout": 30
            },
            drop_old=True 
        )
        
        save_processed_log(updated_log)
        print(f">>> æˆåŠŸï¼æ•°æ®å·²å†™å…¥ Milvus é›†åˆ: {COLLECTION_NAME}")
        
    except Exception as e:
        print(f"\n>>> [é”™è¯¯] æ¨é€å¤±è´¥: {e}")
        # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œæ‰“å°æ›´è¯¦ç»†çš„æç¤º
        if "connect" in str(e).lower():
            print("\nå»ºè®®æ’æŸ¥æ­¥éª¤:")
            print(f"1. ç»ˆç«¯æ‰§è¡Œ: nc -zv {MILVUS_HOST} {MILVUS_PORT}")
            print("2. ç¡®ä¿ VPN å·²å½»åº•å…³é—­")
            print("3. ç¡®ä¿ Docker å®¹å™¨æ­£åœ¨è¿è¡Œ (docker ps)")

if __name__ == "__main__":
    build_index()