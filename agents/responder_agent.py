'''
Author: Yunpeng Shi y.shi27@newcastle.ac.uk
Date: 2026-01-27 10:57:25
LastEditors: Yunpeng Shi y.shi27@newcastle.ac.uk
LastEditTime: 2026-01-28 11:00:51
FilePath: /01/agents/responder_agent.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

from state import agentState
from utils import complete_current_task, llm


async def responder_agent(state: agentState):
    print("   [Responder] æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆå›å¤...")
    
    # 1. è·å–æ‰€æœ‰å­æ™ºèƒ½ä½“çš„åŠ³åŠ¨æˆæœ
    results = state.get("task_results", {})
    
    # 2. è·å–ç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼ˆç”¨äºç»™ LLM æä¾›ä¸Šä¸‹æ–‡ï¼‰
    # æˆ‘ä»¬å€’åºæŸ¥æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    original_input = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            original_input = msg.content
            break

    # 3. æ‹¼æ¥ä¸Šä¸‹æ–‡ç»™ LLM
    context_str = "\n".join([f"ã€{k}çš„å¤„ç†ç»“æœã€‘: {v}" for k, v in results.items()])
    
    # 4. ç¼–å†™ Prompt
    prompt = f"""
    ä½ æ˜¯ä¸€å**èµ„æ·±ä¸”çƒ­å¿ƒ**çš„åœ°é“å®¢æœç»ç†ã€‚
    ä½ çš„æœåŠ¡å®—æ—¨æ˜¯ï¼š**æä¾›è¶…å‡ºç”¨æˆ·é¢„æœŸçš„è¯¦ç»†è§£ç­”ï¼Œç»å¯¹ä¸æ•·è¡ã€‚**
    
    ç”¨æˆ·çš„åŸå§‹é—®é¢˜æ˜¯ï¼š"{original_input}"
    
    ä½ çš„å„éƒ¨é—¨åŒäº‹å·²ç»ç»™å‡ºäº†å¤„ç†ç»“æœï¼Œè¯·ä½ æ±‡æ€»è¿™äº›ä¿¡æ¯ï¼Œç»™ç”¨æˆ·ä¸€ä¸ª**è¿è´¯ã€äº²åˆ‡ã€ç»“æ„æ¸…æ™°**çš„æœ€ç»ˆå›å¤ã€‚
    
    ã€å„éƒ¨é—¨å¤„ç†ç»“æœã€‘ï¼š
    {context_str}
    
    ã€è¦æ±‚ã€‘ï¼š
    1. **æ‰©å†™ä¸æ¶¦è‰²ï¼ˆå…³é”®ï¼‰**ï¼šä¸è¦åªæ˜¯ç®€å•æ‹¼æ¥ç´ æã€‚è¯·å¯¹æ¯ä¸€æ¡ä¿¡æ¯è¿›è¡Œ**å±•å¼€è¯´æ˜**ã€‚
       - å¦‚æœæ˜¯ç¦æ­¢çš„ï¼Œè¯·æ¸©å’Œåœ°è§£é‡ŠåŸå› ï¼ˆå¦‚ï¼šä¸ºäº†å®‰å…¨ã€ä¸ºäº†ç§©åºï¼‰ã€‚
       - å¦‚æœæ˜¯å…è®¸çš„ï¼Œè¯·è¡¥å……ä¸€äº›æ¸©é¦¨æç¤ºï¼ˆå¦‚ï¼šæŠ˜å å¥½ã€æ³¨æ„å°ºå¯¸ï¼‰ã€‚
    2. **ä¿ç•™æ¥æºæ ‡è®°**ï¼šç´ æä¸­å‡ºç°çš„ `ã€ğŸ“šçŸ¥è¯†åº“ã€‘` æ ‡è®°å¿…é¡»ä¿ç•™åœ¨å¯¹åº”å¥å­çš„æœ«å°¾ï¼Œä½œä¸ºæƒå¨å‡­è¯ã€‚
    3. **å¢åŠ å…³æ€€è¯­**ï¼šåœ¨å›å¤çš„å¼€å¤´å’Œç»“å°¾ï¼ŒåŠ å…¥æ›´æœ‰æ¸©åº¦çš„å¯’æš„å’Œå…³æ€€ï¼ˆå¦‚å¤©æ°”æé†’ã€å‡ºè¡Œå®‰å…¨æç¤ºï¼‰ï¼Œè®©å›å¤çœ‹èµ·æ¥ä¸åƒæœºå™¨äººã€‚
    4. **ç»“æ„æ¸…æ™°**ï¼šå†…å®¹è¾ƒå¤šæ—¶ï¼Œè¯·ä½¿ç”¨åˆ†ç‚¹é™ˆè¿°ã€‚
    è¯·ç”Ÿæˆä¸€ä»½**å­—æ•°é€‚ä¸­åé•¿ã€è¯­æ°”æ¸©æš–**çš„å®Œæ•´å›å¤ã€‚
    """
    
    # 5. è°ƒç”¨æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå›å¤
    final_response = await llm.ainvoke([SystemMessage(content=prompt)])
    
    # 6. è¿”å›ç»“æœï¼Œå¹¶é¡ºä¾¿æ¸…ç©ºçœ‹æ¿å’Œç»“æœæ± ï¼ˆä¸ºä¸‹ä¸€è½®å¯¹è¯é‡ç½®çŠ¶æ€ï¼‰
    return {
        "messages": [AIMessage(content=final_response.content, name="final_responder")],
        "task_board": [],     # æ¸…ç©ºçœ‹æ¿
        "task_results": {}    # æ¸…ç©ºç»“æœæ± 
    }
