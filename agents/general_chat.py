import os
from typing import Annotated, List, TypedDict

from langchain_core.messages import (AIMessage, BaseMessage, HumanMessage,
                                     SystemMessage)
from langchain_core.tools import tool
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_milvus import Milvus
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

from state import agentState
from utils import complete_current_task, llm

# ==========================================
# 1. è¿æ¥ Milvus å‘é‡æ•°æ®åº“
# ==========================================

# --- 1.1 ç¯å¢ƒæ¸…ç† (ä¸ build_knowledge.py ä¿æŒä¸€è‡´) ---
for key in ["http_proxy", "https_proxy", "all_proxy", "HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "grpc_proxy", "GRPC_PROXY"]:
    if key in os.environ:
        del os.environ[key]
# ç¡®ä¿ç›´è¿ Docker
os.environ["NO_PROXY"] = "localhost,127.0.0.1,0.0.0.0,::1"

# --- 1.2 å…³é”®ä¿®æ­£ï¼šé…ç½®å‚æ•° ---
# ä¿®æ­£ 1: ä½¿ç”¨ tcp:// åè®® (åˆšæ‰éªŒè¯æˆåŠŸçš„)
MILVUS_URI = "tcp://127.0.0.1:29530" 
COLLECTION_NAME = "metro_knowledge"
# ä¿®æ­£ 2: æŒ‡å‘æœ¬åœ°æ¨¡å‹è·¯å¾„ (ç¡®ä¿ç¦»çº¿å¯ç”¨)
LOCAL_MODEL_PATH = "./models/bge-small-zh-v1.5"

print(f">>> [General Chat] æ­£åœ¨åˆå§‹åŒ–... (Milvus: {MILVUS_URI}, Model: {LOCAL_MODEL_PATH})")

try:
    # åˆå§‹åŒ– Embedding (ä½¿ç”¨æœ¬åœ°æ¨¡å‹)
    embeddings = HuggingFaceEmbeddings(
        model_name=LOCAL_MODEL_PATH,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # åˆå§‹åŒ–å‘é‡åº“è¿æ¥
    vector_store = Milvus(
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME,
        connection_args={
            "uri": MILVUS_URI,
            "token": "",
            "timeout": 30
        },
        auto_id=True
    )
    print(">>> [General Chat] RAG ç»„ä»¶åŠ è½½æˆåŠŸï¼")
    
except Exception as e:
    print(f">>> âŒ [General Chat] åˆå§‹åŒ–å¤±è´¥: {e}")
    # è¿™é‡Œå¯ä»¥æŠ›å‡ºå¼‚å¸¸ï¼Œæˆ–è€…è®©åç»­å·¥å…·è°ƒç”¨æ—¶æŠ¥é”™
    vector_store = None


# ==========================================
# 2. å®šä¹‰å·¥å…· (Tools)
# ==========================================
@tool
def lookup_policy(query: str) -> str:
    """
    ç”¨äºæŸ¥è¯¢åœ°é“ç›¸å…³çš„è§„ç« åˆ¶åº¦ã€ä¹˜å®¢å®ˆåˆ™ã€ç¦æ­¢æºå¸¦ç‰©å“ã€ç¥¨åŠ¡æ”¿ç­–ç­‰å®˜æ–¹æ–‡æ¡£ã€‚
    å½“ç”¨æˆ·çš„é—®é¢˜æ¶‰åŠå…·ä½“è§„å®šæˆ–æ”¿ç­–æ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·ã€‚
    """
    if not vector_store:
        return "ç³»ç»Ÿé”™è¯¯ï¼šçŸ¥è¯†åº“æœªæ­£ç¡®åˆå§‹åŒ–ã€‚"

    try:
        # æ£€ç´¢æœ€ç›¸å…³çš„ 3 ä¸ªç‰‡æ®µ
        retriever = vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"k": 5, "score_threshold": 0.4}
        )
        docs = retriever.invoke(query)
        
        if not docs:
            return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³è§„å®šã€‚"
        
        # ä¼˜åŒ–è¿”å›æ ¼å¼ï¼Œå¢åŠ æ¥æºæ–‡ä»¶å
        results = []
        for i, doc in enumerate(docs):
            source = doc.metadata.get("source_filename", "æœªçŸ¥æ¥æº")
            content = doc.page_content.replace('\n', ' ')
            results.append(f"ã€æ¡æ¬¾ {i+1}ã€‘(æ¥æº: {source}): {content}")
            
        return "\n\n".join(results)
        
    except Exception as e:
        return f"ç³»ç»Ÿé”™è¯¯ï¼šæ— æ³•è¿æ¥çŸ¥è¯†åº“æœåŠ¡å™¨ ({str(e)})ã€‚è¯·è”ç³»ç®¡ç†å‘˜ã€‚"

# å°†å·¥å…·æ”¾å…¥åˆ—è¡¨
tools = [lookup_policy]


# ==========================================
# 3. æ‰‹åŠ¨æ„å»º ReAct å­å›¾ (Sub-Graph)
# ==========================================
class SubAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

llm_with_tools = llm.bind_tools(tools)

def call_model(state: SubAgentState):
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

rag_workflow = StateGraph(SubAgentState)
rag_workflow.add_node("agent", call_model)
rag_workflow.add_node("tools", ToolNode(tools))
rag_workflow.add_edge(START, "agent")
rag_workflow.add_conditional_edges("agent", tools_condition)
rag_workflow.add_edge("tools", "agent")
rag_app = rag_workflow.compile()

# ==========================================
# 4. ä¸»å‡½æ•°
# ==========================================
async def general_chat(state: agentState):
    board = state.get("task_board", [])
    isolated_input = ""
    for task in board:
        if task['task_type'] == "general_chat" and task['status'] == 'pending':
            isolated_input = task['input_content']
            break
            
    if not isolated_input:
        return {"task_board": board}

    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªäº²åˆ‡ã€ä¸“ä¸šçš„åœ°é“ç»¼åˆæœåŠ¡åŠ©æ‰‹ã€‚
    ä½ çš„ä¸»è¦èŒè´£æ˜¯é™ªä¹˜å®¢é—²èŠï¼Œæˆ–è€…è§£ç­”ä¸€äº›é€šç”¨çš„åœ°é“æ”¿ç­–é—®é¢˜ã€‚

    ç­–ç•¥ï¼š
    1. å¦‚æœç”¨æˆ·åªæ˜¯æ‰“æ‹›å‘¼æˆ–é—²èŠï¼Œç›´æ¥å›å¤ï¼Œä¸è¦è°ƒç”¨å·¥å…·ã€‚
    2. å¦‚æœç”¨æˆ·é—®åˆ°å…·ä½“çš„è§„å®šã€æ”¿ç­–ï¼ˆå¦‚æºå¸¦ç‰©å“ã€ä¹˜è½¦è§„åˆ™ç­‰ï¼‰ï¼Œ**å¿…é¡»è°ƒç”¨ lookup_policy å·¥å…·**ã€‚
    3. ä¸¥æ ¼æ ¹æ®å·¥å…·è¿”å›çš„ä¿¡æ¯å›ç­”ã€‚å¦‚æœçŸ¥è¯†åº“è¯´æ²¡æ‰¾åˆ°ï¼Œå°±å‘Šè¯‰ç”¨æˆ·æš‚ä¸æ¸…æ¥šã€‚

    ã€å…³é”®è¦æ±‚ã€‘ï¼š
    4. **å¼•ç”¨æ ‡è®°**ï¼šå½“ä½ ä½¿ç”¨ `lookup_policy` è¿”å›çš„ä¿¡æ¯å›ç­”é—®é¢˜æ—¶ï¼Œå¿…é¡»åœ¨ç›¸å…³å¥å­çš„æœ«å°¾åŠ ä¸Š `ã€ğŸ“šçŸ¥è¯†åº“ã€‘` æ ‡è®°ã€‚
       - é”™è¯¯ç¤ºèŒƒï¼šæŠ˜å è‡ªè¡Œè½¦å¯ä»¥å¸¦ã€‚
       - æ­£ç¡®ç¤ºèŒƒï¼šæŠ˜å è‡ªè¡Œè½¦åœ¨æŠ˜å åç¬¦åˆè¡Œæå°ºå¯¸è¦æ±‚çš„æƒ…å†µä¸‹æ˜¯å¯ä»¥æºå¸¦çš„ã€ğŸ“šçŸ¥è¯†åº“ã€‘ã€‚
    5. å¦‚æœå·¥å…·è¿”å›æœªæ‰¾åˆ°ï¼Œè¯·è¯šå®å›ç­”æœªæ‰¾åˆ°ï¼Œä¸è¦åŠ æ ‡è®°ã€‚
    6.**é€šä¿—åŒ–è§£é‡Š**ï¼šå¦‚æœæ¡æ¬¾æ¯”è¾ƒç”Ÿç¡¬ï¼Œè¯·ç”¨å¤§ç™½è¯è§£é‡Šç»™ä¹˜å®¢å¬ï¼Œè®©ä¿¡æ¯é‡æ›´ä¸°å¯Œã€‚
    """

    inputs = {
        "messages": [
            SystemMessage(content=system_prompt),
            HumanMessage(content=isolated_input)
        ]
    }
    
    # è¿è¡Œå­å›¾
    result = await rag_app.ainvoke(inputs)
    final_content = result["messages"][-1].content
    updated_board = complete_current_task(state, "general_chat")

    return {
        "messages": [AIMessage(content=final_content, name="general_chat")],
        "task_board": updated_board,
        "task_results": {"general_chat": final_content}
    }