'''
Author: Yunpeng Shi y.shi27@newcastle.ac.uk
FilePath: /01/agents/general_chat.py
Description: å¹¶è¡ŒåŒ–æ”¹é€ ç‰ˆ - ä¿®å¤ TypedDict è°ƒç”¨è¡¨è¾¾å¼æŠ¥é”™ (æœ€ç»ˆç¨³å®šç‰ˆ)
'''
import os
from typing import Annotated, List, TypedDict

from langchain_core.messages import (AIMessage, BaseMessage, HumanMessage,
                                     SystemMessage)
from langchain_core.tools import tool
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_milvus import Milvus
from langgraph.graph import START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from state import WorkerState
from utils import llm, update_task_result

# --- 1.1 ç¯å¢ƒæ¸…ç† ---
for key in ["http_proxy", "https_proxy", "all_proxy", "HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "grpc_proxy", "GRPC_PROXY"]:
    if key in os.environ:
        del os.environ[key]
os.environ["NO_PROXY"] = "localhost,127.0.0.1,0.0.0.0,::1"

# --- 1.2 é…ç½®å‚æ•° ---
MILVUS_URI = "tcp://127.0.0.1:29530" 
COLLECTION_NAME = "metro_knowledge"
LOCAL_MODEL_PATH = "./models/bge-small-zh-v1.5"

print(f">>> [General Chat] æ­£åœ¨åˆå§‹åŒ–... (Milvus: {MILVUS_URI})")

try:
    embeddings = HuggingFaceEmbeddings(
        model_name=LOCAL_MODEL_PATH,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    vector_store = Milvus(
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME,
        connection_args={
            "uri": MILVUS_URI,
            "token": "",
            "timeout": 30
        },
        index_params={"metric_type": "L2", "index_type": "HNSW", "params": {"M": 8, "efConstruction": 64}},
        auto_id=True
    )
    print(">>> [General Chat] RAG ç»„ä»¶åŠ è½½æˆåŠŸï¼")
    
except Exception as e:
    print(f">>> âŒ [General Chat] åˆå§‹åŒ–å¤±è´¥: {e}")
    vector_store = None

@tool
def lookup_policy(query: str) -> str:
    """æŸ¥è¯¢åœ°é“ç›¸å…³è§„ç« åˆ¶åº¦ã€ä¹˜è½¦å®ˆåˆ™ç­‰å®˜æ–¹æ–‡æ¡£ã€‚"""
    if not vector_store:
        return "ç³»ç»Ÿé”™è¯¯ï¼šçŸ¥è¯†åº“æœªæ­£ç¡®åˆå§‹åŒ–ã€‚"

    try:
        retriever = vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 5, 
                "score_threshold": 0.4,
                "param": {"metric_type": "L2", "nprobe": 10} 
            }
        )
        docs = retriever.invoke(query)
        
        if not docs:
            return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³è§„å®šã€‚"
        
        results = []
        for i, doc in enumerate(docs):
            source = doc.metadata.get('source_filename', 'æœªçŸ¥')
            # é¿å¼€ f-string åæ–œæ é™åˆ¶
            clean_content = doc.page_content.replace('\n', ' ')
            results.append(f"ã€æ¡æ¬¾ {i+1}ã€‘(æ¥æº: {source}): {clean_content}")
            
        return "\n\n".join(results)
    except Exception as e:
        return f"ç³»ç»Ÿé”™è¯¯ï¼šæ— æ³•è¿æ¥çŸ¥è¯†åº“æœåŠ¡å™¨ ({str(e)})ã€‚"

# --- 2. ReAct å­å›¾å®šä¹‰ (ä¿®å¤æŠ¥é”™çš„å…³é”®éƒ¨åˆ†) ---

tools = [lookup_policy]
llm_with_tools = llm.bind_tools(tools)

# ã€ä¿®å¤ã€‘æ˜¾å¼å®šä¹‰ TypedDict ç±»ï¼Œè€Œä¸æ˜¯åœ¨å‡½æ•°å‚æ•°é‡Œè°ƒç”¨æ„é€ å‡½æ•°
class SubAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

def call_model(state: SubAgentState):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# ã€ä¿®å¤ã€‘ä½¿ç”¨å®šä¹‰å¥½çš„ç±»
rag_workflow = StateGraph(SubAgentState)
rag_workflow.add_node("agent", call_model)
rag_workflow.add_node("tools", ToolNode(tools))
rag_workflow.add_edge(START, "agent")
rag_workflow.add_conditional_edges("agent", tools_condition)
rag_workflow.add_edge("tools", "agent")
rag_app = rag_workflow.compile()

# --- 3. ä¸»å‡½æ•° ---

async def general_chat(state: WorkerState):
    task = state["task"]
    isolated_input = task['input_content']

    # ã€æ ¸å¿ƒé€»è¾‘ã€‘è·å–å¹¶å¤„ç†å†å²
    global_messages = state.get("messages", [])
    print(f"[General] æ­£åœ¨å¤„ç† (RAGå·²å¯ç”¨): {isolated_input}")

    # æŠ€å·§ï¼šå…¨å±€å†å²çš„æœ€åä¸€æ¡é€šå¸¸æ˜¯ç”¨æˆ·æœ¬è½®çš„â€œå¤æ‚æŒ‡ä»¤â€ï¼ˆè¢« Supervisor æ‹†è§£å‰çš„ï¼‰ã€‚
    # ä¸ºäº†è®© Worker ä¸“æ³¨å¤„ç† isolated_inputï¼Œæˆ‘ä»¬é€šå¸¸å–â€œä¸Šä¸€è½®ä¸ºæ­¢çš„å†å²â€ä½œä¸º Contextã€‚
    history_context = global_messages[:-1] if global_messages else []

    # å¼ºåŒ– Prompt (Few-Shot)
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªäº²åˆ‡ã€ä¸“ä¸šçš„åœ°é“ç»¼åˆæœåŠ¡åŠ©æ‰‹ã€‚
    ä½ çš„ä¸»è¦èŒè´£æ˜¯é™ªä¹˜å®¢é—²èŠï¼Œæˆ–è€…ä¾æ®çœŸå®è§„å®šè§£ç­”åœ°é“æ”¿ç­–é—®é¢˜ã€‚

    ### æ ¸å¿ƒæŒ‡ä»¤ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
    1. **å¿…é¡»æŸ¥è¯**ï¼šå½“ç”¨æˆ·é—®åˆ°å…·ä½“çš„è§„å®šã€æ”¿ç­–ï¼ˆå¦‚æºå¸¦ç‰©å“ã€å®‰æ£€ã€ç¥¨åŠ¡è§„åˆ™ï¼‰æ—¶ï¼Œ**å¿…é¡»è°ƒç”¨ lookup_policy å·¥å…·**ã€‚
    2. **å¼ºåˆ¶æ ‡è®°**ï¼šå‡¡æ˜¯ä½ çš„å›ç­”ä¸­å¼•ç”¨äº† `lookup_policy` å·¥å…·è¿”å›çš„ä¿¡æ¯ï¼Œ**å¿…é¡»**åœ¨å¯¹åº”çš„å¥å­æœ«å°¾åŠ ä¸Š `ã€ğŸ“šçŸ¥è¯†åº“ã€‘` æ ‡è®°ã€‚è¿™æ˜¯ä¸ºäº†å‘ç”¨æˆ·è¯æ˜ä¿¡æ¯çš„æƒå¨æ€§ã€‚
    
    ### ç¤ºä¾‹å­¦ä¹ ï¼ˆè¯·æ¨¡ä»¿ï¼‰ï¼š
    - ç”¨æˆ·ï¼šèƒ½å¸¦ç™½é…’è¿›ç«™å—ï¼Ÿ
    - å·¥å…·è¿”å›ï¼š...50åº¦ä»¥ä¸Šæ•£è£…ç™½é…’ç¦æ­¢æºå¸¦...
    - âŒ é”™è¯¯å›ç­”ï¼šæ ¹æ®è§„å®šï¼Œæ•£è£…çš„é«˜æµ“åº¦ç™½é…’æ˜¯ä¸è®©å¸¦çš„ã€‚
    - âœ… æ­£ç¡®å›ç­”ï¼šä¸ºæ‚¨æŸ¥è¯¢äº†ç›¸å…³è§„å®šï¼Œ50åº¦ä»¥ä¸Šçš„æ•£è£…ç™½é…’æ˜¯ç¦æ­¢æºå¸¦è¿›ç«™çš„ã€ğŸ“šçŸ¥è¯†åº“ã€‘ã€‚

    3. **è¯šå®åŸåˆ™**ï¼šå¦‚æœå·¥å…·æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å‘Šè¯‰ç”¨æˆ·â€œæš‚æœªæŸ¥åˆ°ç›¸å…³è§„å®šâ€ï¼Œè¿™ç§æƒ…å†µä¸‹**ä¸éœ€è¦**åŠ æ ‡è®°ã€‚
    """

    # ã€æ„é€ è¾“å…¥ã€‘ System + å†å²Context + å½“å‰çº¯å‡€æŒ‡ä»¤
    inputs = {
        "messages": [SystemMessage(content=system_prompt)] + history_context + [HumanMessage(content=isolated_input)]
    }
    result = await rag_app.ainvoke(inputs)
    final_content = result["messages"][-1].content
    
    return {
        "messages": [AIMessage(content=final_content, name="general_chat")],
        "task_board": [update_task_result(task, result=final_content)]
    }